{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23zmVykztHsK"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rxYdl2ms4RC"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q accelerate\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q torch torchvision transformers scikit-learn accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJi1blFZyDAi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoModelForImageClassification\n",
        "from transformers import AutoConfig, AutoModelForImageClassification\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "import torch.optim as optim # import optim\n",
        "from torch.amp import autocast, GradScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from umap import UMAP  # pip install umap-learn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOJ_3J3bvBs4"
      },
      "outputs": [],
      "source": [
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed to ensure reproducible results\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "NUkbthZ69DYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3N6CGxCwZHA"
      },
      "source": [
        "# FINE-TUEN with Scheduler (Freeze Backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-PvYOR162I0"
      },
      "outputs": [],
      "source": [
        "#transform （keep in PIL foramt)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda x: x.convert(\"RGB\"))  # Make sure it is in PIL format\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d832tAO1-apz"
      },
      "outputs": [],
      "source": [
        "# Customize collate_fn to keep the image format as is\n",
        "def collate_pil(batch):\n",
        "    images, labels = zip(*batch)\n",
        "    return list(images), torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN3VYh9d74N1"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "# dataset download\n",
        "full_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "val_ratio = 0.1\n",
        "val_size = int(len(full_train) * val_ratio)\n",
        "train_size = len(full_train) - val_size\n",
        "\n",
        "train_set, val_set = random_split(full_train, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qTA489FxbnA"
      },
      "outputs": [],
      "source": [
        "# try batch size as 128, and use num_worker to accelerate\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate_pil, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, collate_fn=collate_pil, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate_pil, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9pNcz90wgED"
      },
      "outputs": [],
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load processor from base model (not fine-tuned one, because processor没变)\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "\n",
        "# Load base DINOv2 model structure (100-class head)\n",
        "# This model instance is set up for 100 classes, which is correct for CIFAR-100\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"facebook/dinov2-base\",\n",
        "    num_labels=100,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Load fine-tuned weights from CIFAR-10 training\n",
        "# Load the state_dict first\n",
        "cifar10_state_dict = torch.load(\"/content/drive/MyDrive/rec_model/dinov2/dinov2_finetuned_cifar10.pth\")\n",
        "\n",
        "# Create a new state_dict excluding the classifier weights\n",
        "# This is necessary because the saved state_dict's classifier is for 10 classes\n",
        "# while the current model's classifier is for 100 classes.\n",
        "filtered_state_dict = {k: v for k, v in cifar10_state_dict.items() if 'classifier' not in k}\n",
        "\n",
        "# Load the filtered state_dict into the model\n",
        "# strict=False is still useful here in case of other minor mismatches,\n",
        "# but filtering the classifier is the key step for this specific error.\n",
        "model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "# Freeze selected layers BEFORE defining optimizer\n",
        "for name, param in model.named_parameters():\n",
        "    if \"encoder.layer\" in name:\n",
        "        layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
        "        if layer_num < 9:\n",
        "            param.requires_grad = False\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5) # use optim.AdamW\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize GradScaler for automatic mixed precision\n",
        "scaler = GradScaler() # Add this line to initialize GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5q8aeBh8L9d"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(**inputs)\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkN-GRyDyWg9"
      },
      "outputs": [],
      "source": [
        "best_val_acc = 0.0\n",
        "no_improvement = 0\n",
        "early_stop_patience = 5  # Set patience as 5 to prevent early stopping\n",
        "save_path = \"/content/drive/MyDrive/cifar100model/dinov2_finetuned_cifar100.pth\"\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "lr_history = []\n",
        "\n",
        "for epoch in range(40): # Beacause I already had early stopping, I used 40 to try\n",
        "    model.train()\n",
        "    # Initialize\n",
        "    total_loss = 0\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(\"cuda\"):\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Train accuracy tracking\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    # Log values\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    scheduler.step(val_acc)\n",
        "    lr_history.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"[Epoch {epoch+1}] Current LR: {current_lr:.6f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        print(f\"New best val acc! Saving model to {save_path}\")\n",
        "        best_val_acc = val_acc\n",
        "        no_improvement = 0\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    else:\n",
        "        no_improvement += 1\n",
        "        print(f\"No improvement for {no_improvement} epoch(s).\")\n",
        "\n",
        "    if no_improvement >= early_stop_patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZQYMwF_DEth"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfBg-qUhbb34"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dWolp7Z8qA3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5XV95k3f136"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "log_df = pd.DataFrame({\n",
        "    \"epoch\": list(range(1, len(train_losses)+1)),\n",
        "    \"train_loss\": train_losses,\n",
        "    \"train_acc\": train_accuracies,\n",
        "    \"val_acc\": val_accuracies,\n",
        "    \"lr\": lr_history\n",
        "})\n",
        "\n",
        "log_df.to_csv(\"/content/drive/MyDrive/cifar100model/dinov2_training_log.csv\", index=False)\n",
        "log_df.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}